{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Setting Up a Spark Session and Working with Delta Lake\n",
    "\n",
    "**Objective**: Learn how to set up a Spark session with Delta Lake support, and how to save and read data from Delta Lake.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Apache Spark with Delta Lake installed.\n",
    "- Jupyter Notebook or any Python IDE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n",
    "\n",
    "Ensure you have the `delta-spark` package installed, which provides Delta Lake integration with PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install delta-spark package if not already installed\n",
    "!pip install delta-spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Initialize Spark Session\n",
    "\n",
    "Import the necessary libraries and initialize a Spark session with Delta Lake support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session with Delta Lake configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Sample DataFrame\n",
    "\n",
    "Let's create a simple DataFrame to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(\"Alice\", 34), (\"Bob\", 36), (\"Cathy\", 30)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write Data to Delta Lake\n",
    "\n",
    "Write the DataFrame to a Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Delta table path\n",
    "delta_table_path = \"/tmp/delta-table\"\n",
    "\n",
    "# Write DataFrame to Delta format\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Read Data from Delta Lake\n",
    "\n",
    "Read the data back from the Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta table\n",
    "df_delta = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Show Delta table data\n",
    "df_delta.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Update Data in Delta Lake\n",
    "\n",
    "Update records in the Delta table using Delta Lake's `update` functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create DeltaTable object\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Update age where Name is 'Alice'\n",
    "delta_table.update(\n",
    "    condition=col(\"Name\") == \"Alice\",\n",
    "    set={\"Age\": col(\"Age\") + 1}\n",
    ")\n",
    "\n",
    "# Show updated data\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Time Travel in Delta Lake\n",
    "\n",
    "Delta Lake allows you to query older snapshots of data using time travel.\n",
    "\n",
    "### View Table History\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Delta table history\n",
    "delta_table.history().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Previous Versions\n",
    "\n",
    "Read data from version 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from version 0\n",
    "df_version0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "\n",
    "# Show data from version 0\n",
    "df_version0.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Append New Data to Delta Lake\n",
    "\n",
    "Append new records to the Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data to append\n",
    "new_data = [(\"David\", 29), (\"Eva\", 32)]\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Append to Delta table\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "# Show updated table\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Delete Data from Delta Lake\n",
    "\n",
    "Delete records from the Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete where Age is less than 32\n",
    "delta_table.delete(condition=col(\"Age\") < 32)\n",
    "\n",
    "# Show data after deletion\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Vacuum Old Data\n",
    "\n",
    "Clean up old snapshots and remove unused files with `VACUUM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacuum Delta table (Note: Only run if you are sure)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "delta_table.vacuum(0)\n",
    "\n",
    "# Try to time travel to version 0 (should fail if data is vacuumed)\n",
    "try:\n",
    "    df_version0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "    df_version0.show()\n",
    "except Exception as e:\n",
    "    print(\"Time travel failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the Spark Session\n",
    "\n",
    "After completing your tasks, don't forget to stop the Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
